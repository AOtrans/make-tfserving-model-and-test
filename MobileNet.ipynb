{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "config = tf.ConfigProto()  \n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img): \n",
    "    width = img.size[0]\n",
    "    height = img.size[1]\n",
    "    \n",
    "    scale = random.uniform(0.85, 1.0)\n",
    "    px = random.uniform(0.0, 1.0 - scale)*width\n",
    "    py = random.uniform(0.0, 1.0 - scale)*height\n",
    "    \n",
    "    \n",
    "    img = img.crop((int(px), int(py), int(px+scale*width)-1, int(py+scale*height)-1 )) \n",
    "    return img \n",
    "\n",
    "def random_flip_left_right(img): \n",
    "    prob = random.randint(0,1) \n",
    "    if prob == 1: \n",
    "        img = img.transpose(Image.FLIP_LEFT_RIGHT) \n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def mk_generator(batchsize, file_path, shuffle = True, train = True):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        if shuffle:\n",
    "            random.shuffle(lines)\n",
    "        \n",
    "        datas = []\n",
    "        labels = []\n",
    "        imgPaths = []\n",
    "        \n",
    "        i=0\n",
    "        \n",
    "        while True:\n",
    "            line = lines[i%len(lines)]\n",
    "            \n",
    "            paras = line.strip('\\n').split(' ')\n",
    "            imagePath = paras[0]\n",
    "            label = paras[1]\n",
    "            img = Image.open(imagePath)\n",
    "            if train:\n",
    "                img = random_flip_left_right(img)\n",
    "                img = random_crop(img)\n",
    "\n",
    "            data = np.array(img.resize((224,224)), dtype = np.float32)/127.5 - 1. #FROM keras file\n",
    "            \n",
    "            \n",
    "            datas.append(data)\n",
    "            labels.append(label)\n",
    "            imgPaths.append(line)\n",
    "            \n",
    "            i+=1\n",
    "            \n",
    "            if (i%batchsize) == 0:\n",
    "                if train:\n",
    "                    yield np.array(datas), np.array(to_categorical(labels, 2))\n",
    "                else:\n",
    "                    yield np.array(datas), np.array(to_categorical(labels, 2)), imgPaths\n",
    "                    \n",
    "                datas = []\n",
    "                labels = []\n",
    "                imgPaths = []\n",
    "                \n",
    "            if i % len(lines) == 0 and shuffle:\n",
    "                print 'shuffle'\n",
    "                random.shuffle(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "436/446 [============================>.] - ETA: 1s - loss: 0.0347 - acc: 0.9913shuffle\n",
      "445/446 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9915\n",
      "Epoch 00001: saving model to mobilenet.h5\n",
      "446/446 [==============================] - 87s 196ms/step - loss: 0.0343 - acc: 0.9915\n",
      "Epoch 2/10\n",
      "437/446 [============================>.] - ETA: 1s - loss: 0.0242 - acc: 0.9944shuffle\n",
      "445/446 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.9944\n",
      "Epoch 00002: saving model to mobilenet.h5\n",
      "446/446 [==============================] - 79s 177ms/step - loss: 0.0244 - acc: 0.9943\n",
      "Epoch 3/10\n",
      "438/446 [============================>.] - ETA: 1s - loss: 0.0253 - acc: 0.9939shuffle\n",
      "445/446 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9939\n",
      "Epoch 00003: saving model to mobilenet.h5\n",
      "446/446 [==============================] - 80s 178ms/step - loss: 0.0253 - acc: 0.9939\n",
      "Epoch 4/10\n",
      " 48/446 [==>...........................] - ETA: 1:11 - loss: 0.0226 - acc: 0.9954"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-176db85a40e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msavemodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mobilenet.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msavemodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_generator.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_samples = 14299\n",
    "\n",
    "K.set_learning_phase(1)\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), alpha=1.4, include_top=False)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.kernel_regularizer = keras.regularizers.l2(0.05)\n",
    "    #layer.trainable = False\n",
    "\n",
    "inputs = base_model.input\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "y = keras.layers.Dense(2, activation='softmax',kernel_regularizer = keras.regularizers.l2(0.05))(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = y)\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('./tt_mobilenet.h5', by_name=True);\n",
    "generator = mk_generator(batchsize=batch_size, file_path = '/home/sw/data/yc/yaochang/2dtrain.txt')\n",
    "\n",
    "savemodel = keras.callbacks.ModelCheckpoint('mobilenet.h5', verbose=1)\n",
    "model.fit_generator(generator, num_samples/batch_size, epochs=10, callbacks=[savemodel])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/sw/data/1101-tag/1/1_1541052336_26f01cea-dd9c-11e8-ad10-6045cb7fb916/0004.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541052336_26f01cea-dd9c-11e8-ad10-6045cb7fb916/0002.jpg 1\\n']\n",
      "0.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053456_c24da714-dd9e-11e8-ad10-6045cb7fb916/0001.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053456_c24da714-dd9e-11e8-ad10-6045cb7fb916/0014.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053661_3c90d0dc-dd9f-11e8-ad10-6045cb7fb916/0012.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053661_3c90d0dc-dd9f-11e8-ad10-6045cb7fb916/0009.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053821_9bbe7899-dd9f-11e8-ad10-6045cb7fb916/0000.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541053821_9bbe7899-dd9f-11e8-ad10-6045cb7fb916/0010.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541054074_32a92385-dda0-11e8-ad10-6045cb7fb916/0015.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541054074_32a92385-dda0-11e8-ad10-6045cb7fb916/0003.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541054864_0955be1e-dda2-11e8-ad10-6045cb7fb916/0006.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/1_1541054864_0955be1e-dda2-11e8-ad10-6045cb7fb916/0015.jpg 1\\n']\n",
      "0.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "[]\n",
      "1.0\n",
      "['/home/sw/data/1101-tag/1/1_1541057731_b6ca636e-dda8-11e8-ad10-6045cb7fb916/0005.jpg 1\\n']\n",
      "0.0\n",
      "[]\n",
      "1.0\n",
      "['/home/sw/data/1101-tag/1/2_1541052727_0f958a5c-dd9d-11e8-ad10-6045cb7fb916/0001.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/2_1541052727_0f958a5c-dd9d-11e8-ad10-6045cb7fb916/0005.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/2_1541053827_9f472f78-dd9f-11e8-ad10-6045cb7fb916/0004.jpg 1\\n']\n",
      "0.0\n",
      "[]\n",
      "1.0\n",
      "['/home/sw/data/1101-tag/1/2_1541056698_4e7e93e0-dda6-11e8-ad10-6045cb7fb916/0004.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/2_1541056698_4e7e93e0-dda6-11e8-ad10-6045cb7fb916/0013.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/2_1541058832_46e2dc54-ddab-11e8-ad10-6045cb7fb916/0004.jpg 1\\n']\n",
      "0.0\n",
      "['/home/sw/data/1101-tag/1/2_1541058832_46e2dc54-ddab-11e8-ad10-6045cb7fb916/0010.jpg 1\\n']\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "batch_size =1\n",
    "num_samples = 34\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), alpha=1.4, include_top=False)\n",
    "\n",
    "inputs = base_model.input\n",
    "x = base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "y = keras.layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = y)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.load_weights('mobilenet.h5')\n",
    "\n",
    "generator = mk_generator(batchsize=batch_size, file_path = '/home/sw/data/1101-tag/2dtest.txt', shuffle = False, train = False)\n",
    "\n",
    "for i in range(num_samples/batch_size):\n",
    "    img, label, imgnames = next(generator)\n",
    "    outputs = model.predict(img, batch_size=batch_size)\n",
    "    res = np.equal(np.argmax(outputs, -1),np.argmax(label,-1)).astype(np.float32)\n",
    "    print np.array(imgnames)[np.where(res==0)]\n",
    "    print np.mean(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
