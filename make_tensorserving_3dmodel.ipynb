{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS=3\n",
    "CROP_SIZE=160\n",
    "NUM_FRAMES_PER_CLIP=16\n",
    "BATCH_SIZE=10\n",
    "RGB_CHANNEL=3\n",
    "IS_TRAIN=True\n",
    "BLOCK_EXPANSION=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_weight(name,kshape,wd=0.0005*0.3*0.3):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var=tf.get_variable(name,shape=kshape,initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    if wd!=0:\n",
    "        weight_decay = tf.nn.l2_loss(var)*wd\n",
    "        tf.add_to_collection('weightdecay_losses', weight_decay)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convS(name,l_input,in_channels,out_channels):\n",
    "    return tf.nn.bias_add(tf.nn.conv3d(l_input,get_conv_weight(name=name,\n",
    "                                                               kshape=[1,3,3,in_channels,out_channels]),\n",
    "                                                               strides=[1,1,1,1,1],padding='SAME'),\n",
    "                                              get_conv_weight(name+'_bias',[out_channels],0))\n",
    "def convT(name,l_input,in_channels,out_channels):\n",
    "    return tf.nn.bias_add(tf.nn.conv3d(l_input,get_conv_weight(name=name,\n",
    "                                                               kshape=[3,1,1,in_channels,out_channels]),\n",
    "                                                               strides=[1,1,1,1,1],padding='SAME'),\n",
    "                                              get_conv_weight(name+'_bias',[out_channels],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck():\n",
    "    def __init__(self,l_input,inplanes,planes,stride=1,downsample='',n_s=0,depth_3d=47):\n",
    "        \n",
    "        self.X_input=l_input\n",
    "        self.downsample=downsample\n",
    "        self.planes=planes\n",
    "        self.inplanes=inplanes\n",
    "        self.depth_3d=depth_3d\n",
    "        self.ST_struc=('A','B','C')\n",
    "        self.len_ST=len(self.ST_struc)\n",
    "        self.id=n_s\n",
    "        self.n_s=n_s\n",
    "        self.ST=list(self.ST_struc)[self.id % self.len_ST]\n",
    "        self.stride_p=[1,1,1,1,1]\n",
    "       \n",
    "        if self.downsample!='':\n",
    "            self.stride_p=[1,1,2,2,1]\n",
    "        if n_s<self.depth_3d:\n",
    "            if n_s==0:\n",
    "                self.stride_p=[1,1,1,1,1]\n",
    "        else:\n",
    "            if n_s==self.depth_3d:\n",
    "                self.stride_p=[1,2,2,2,1]\n",
    "            else:\n",
    "                self.stride_p=[1,1,1,1,1]\n",
    "    #P3D has three types of bottleneck sub-structions.\n",
    "    def ST_A(self,name,x):\n",
    "        x=convS(name+'_S',x,self.planes,self.planes)\n",
    "        x=tf.layers.batch_normalization(x,training=IS_TRAIN)\n",
    "        x=tf.nn.relu(x)\n",
    "        x=convT(name+'_T',x,self.planes,self.planes)\n",
    "        x=tf.layers.batch_normalization(x,training=IS_TRAIN)\n",
    "        x=tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def ST_B(self,name,x):\n",
    "        tmp_x=convS(name+'_S',x,self.planes,self.planes)\n",
    "        tmp_x=tf.layers.batch_normalization(tmp_x,training=IS_TRAIN)\n",
    "        tmp_x=tf.nn.relu(tmp_x)\n",
    "        x=convT(name+'_T',x,self.planes,self.planes)\n",
    "        x=tf.layers.batch_normalization(x,training=IS_TRAIN)\n",
    "        x=tf.nn.relu(x)\n",
    "        return x+tmp_x\n",
    "    \n",
    "    def ST_C(self,name,x):\n",
    "        x=convS(name+'_S',x,self.planes,self.planes)\n",
    "        x=tf.layers.batch_normalization(x,training=IS_TRAIN)\n",
    "        x=tf.nn.relu(x)\n",
    "        tmp_x=convT(name+'_T',x,self.planes,self.planes)\n",
    "        tmp_x=tf.layers.batch_normalization(tmp_x,training=IS_TRAIN)\n",
    "        tmp_x=tf.nn.relu(tmp_x)\n",
    "        return x+tmp_x\n",
    "    \n",
    "    def infer(self):\n",
    "        residual=self.X_input\n",
    "        if self.n_s<self.depth_3d:\n",
    "            out=tf.nn.conv3d(self.X_input,get_conv_weight('conv3_{}_1'.format(self.id),[1,1,1,self.inplanes,self.planes]),\n",
    "                             strides=self.stride_p,padding='SAME')\n",
    "            out=tf.layers.batch_normalization(out,training=IS_TRAIN)\n",
    "            \n",
    "        else:\n",
    "            param=self.stride_p\n",
    "            param.pop(1)\n",
    "            out=tf.nn.conv2d(self.X_input,get_conv_weight('conv2_{}_1'.format(self.id),[1,1,self.inplanes,self.planes]),\n",
    "                             strides=param,padding='SAME')\n",
    "            out=tf.layers.batch_normalization(out,training=IS_TRAIN)\n",
    "    \n",
    "        out=tf.nn.relu(out)    \n",
    "        if self.id<self.depth_3d:\n",
    "            if self.ST=='A':\n",
    "                out=self.ST_A('STA_{}_2'.format(self.id),out)\n",
    "            elif self.ST=='B':\n",
    "                out=self.ST_B('STB_{}_2'.format(self.id),out)\n",
    "            elif self.ST=='C':\n",
    "                out=self.ST_C('STC_{}_2'.format(self.id),out)\n",
    "        else:\n",
    "            out=tf.nn.conv2d(out,get_conv_weight('conv2_{}_2'.format(self.id),[3,3,self.planes,self.planes]),\n",
    "                                  strides=[1,1,1,1],padding='SAME')\n",
    "            out=tf.layers.batch_normalization(out,training=IS_TRAIN)\n",
    "            out=tf.nn.relu(out)\n",
    "\n",
    "        if self.n_s<self.depth_3d:\n",
    "            out=tf.nn.conv3d(out,get_conv_weight('conv3_{}_3'.format(self.id),[1,1,1,self.planes,self.planes*BLOCK_EXPANSION]),\n",
    "                             strides=[1,1,1,1,1],padding='SAME')\n",
    "            out=tf.layers.batch_normalization(out,training=IS_TRAIN)\n",
    "        else:\n",
    "            out=tf.nn.conv2d(out,get_conv_weight('conv2_{}_3'.format(self.id),[1,1,self.planes,self.planes*BLOCK_EXPANSION]),\n",
    "                             strides=[1,1,1,1],padding='SAME')\n",
    "            out=tf.layers.batch_normalization(out,training=IS_TRAIN)\n",
    "           \n",
    "        if len(self.downsample)==1:\n",
    "            residual=tf.nn.conv2d(residual,get_conv_weight('dw2d_{}'.format(self.id),[1,1,self.inplanes,self.planes*BLOCK_EXPANSION]),\n",
    "                                  strides=[1,2,2,1],padding='SAME')\n",
    "            residual=tf.layers.batch_normalization(residual,training=IS_TRAIN)\n",
    "        elif len(self.downsample)==2:\n",
    "            residual=tf.nn.conv3d(residual,get_conv_weight('dw3d_{}'.format(self.id),[1,1,1,self.inplanes,self.planes*BLOCK_EXPANSION]),\n",
    "                                  strides=self.downsample[1],padding='SAME')\n",
    "            residual=tf.layers.batch_normalization(residual,training=IS_TRAIN)\n",
    "        out+=residual\n",
    "        out=tf.nn.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_block():\n",
    "    def __init__(self,_X,planes,num,inplanes,cnt,depth_3d=47,stride=1):\n",
    "        self.input=_X\n",
    "        self.planes=planes\n",
    "        self.inplanes=inplanes\n",
    "        self.num=num\n",
    "        self.cnt=cnt\n",
    "        self.depth_3d=depth_3d\n",
    "        self.stride=stride\n",
    "        if self.cnt<depth_3d:\n",
    "            if self.cnt==0:\n",
    "                stride_p=[1,1,1,1,1]\n",
    "            else:\n",
    "                stride_p=[1,1,2,2,1]\n",
    "            if stride!=1 or inplanes!=planes*BLOCK_EXPANSION:\n",
    "                self.downsample=['3d',stride_p]\n",
    "        else:\n",
    "            if stride!=1 or inplanes!=planes*BLOCK_EXPANSION:\n",
    "                self.downsample=['2d']\n",
    "    def infer(self):\n",
    "        x=Bottleneck(self.input,self.inplanes,self.planes,self.stride,self.downsample,n_s=self.cnt,depth_3d=self.depth_3d).infer()\n",
    "        self.cnt+=1\n",
    "        self.inplanes=BLOCK_EXPANSION*self.planes\n",
    "        for i in range(1,self.num):\n",
    "            x=Bottleneck(x,self.inplanes,self.planes,n_s=self.cnt,depth_3d=self.depth_3d).infer()\n",
    "            self.cnt+=1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_p3d(_X,BATCH_SIZE):\n",
    "    cnt=0\n",
    "    conv1_custom=tf.nn.conv3d(_X,get_conv_weight('firstconv1',[1,7,7,RGB_CHANNEL,64]),strides=[1,1,2,2,1],padding='SAME')\n",
    "    conv1_custom_bn=tf.layers.batch_normalization(conv1_custom,training=IS_TRAIN)\n",
    "    conv1_custom_bn_relu=tf.nn.relu(conv1_custom_bn)\n",
    "    x=tf.nn.max_pool3d(conv1_custom_bn_relu,[1,2,3,3,1],strides=[1,2,2,2,1],padding='SAME')\n",
    "    b1=make_block(x,64,3,64,cnt)\n",
    "    x=b1.infer()\n",
    "    cnt=b1.cnt\n",
    "   \n",
    "    x=tf.nn.max_pool3d(x,[1,2,1,1,1],strides=[1,2,1,1,1],padding='SAME')\n",
    "    \n",
    "    b2=make_block(x,128,8,256,cnt,stride=2)\n",
    "    x=b2.infer()\n",
    "    cnt=b2.cnt\n",
    "    x=tf.nn.max_pool3d(x,[1,2,1,1,1],strides=[1,2,1,1,1],padding='SAME')\n",
    "    \n",
    "    b3=make_block(x,256,36,512,cnt,stride=2)\n",
    "    x=b3.infer()\n",
    "    cnt=b3.cnt\n",
    "    x=tf.nn.max_pool3d(x,[1,2,1,1,1],strides=[1,2,1,1,1],padding='SAME')\n",
    "    \n",
    "    shape=x.shape.as_list()\n",
    "    x=tf.reshape(x,shape=[-1,shape[2],shape[3],shape[4]])\n",
    "    \n",
    "    x=make_block(x,512,3,1024,cnt,stride=2).infer()\n",
    "    \n",
    "    #Caution:make sure avgpool on the input which has the same shape as kernelsize has been setted padding='VALID'\n",
    "    x=tf.nn.avg_pool(x,[1,5,5,1],strides=[1,1,1,1],padding='VALID')\n",
    "    \n",
    "    x=tf.reshape(x,shape=[-1,2048])\n",
    "    if(IS_TRAIN):\n",
    "        x=tf.nn.dropout(x,keep_prob=1)\n",
    "    else:\n",
    "        x=tf.nn.dropout(x,keep_prob=1)\n",
    "    \n",
    "    x=tf.layers.dense(x,NUM_CLASS)\n",
    "    \n",
    "    return x,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(name_scope,logit,labels):\n",
    "    cross_entropy_mean=tf.reduce_mean(\n",
    "                    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logit))\n",
    "    tf.summary.scalar(name_scope+'_cross_entropy',\n",
    "                     cross_entropy_mean\n",
    "                     )\n",
    "    weight_decay_loss=tf.get_collection('weightdecay_losses')\n",
    "    \n",
    "    tf.summary.scalar(name_scope+'_weight_decay_loss',tf.reduce_sum(weight_decay_loss))\n",
    "    total_loss=cross_entropy_mean+tf.reduce_sum(weight_decay_loss)\n",
    "    tf.summary.scalar(name_scope+'_total_loss',total_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logit,labels):\n",
    "    correct=tf.equal(tf.argmax(logit,1),labels)\n",
    "    acc=tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVING_AVERAGE_DECAY=0.9\n",
    "MODEL_PATH='./SC_DA_P3D_RES94_23999-23999'\n",
    "USE_PRETRAIN=False\n",
    "MAX_STEPS=40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Make sure IS_TRAIN==True before traininig.\n",
    "print(IS_TRAIN)\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(example_proto): \n",
    "    features = {\"label\": tf.FixedLenFeature([], tf.int64), \n",
    "                \"imgs_bytes\": tf.FixedLenFeature([16], tf.string),\n",
    "                \"shape\": tf.FixedLenFeature([2], tf.int64)} #此处解析格式保证shape和存tfrecord的shape匹配（imgs_bytes为16个jpg压缩存储字节码的list）\n",
    "    parsed_features = tf.parse_single_example(example_proto, features) #按格式解析record\n",
    "    encode_bytes = parsed_features[\"imgs_bytes\"] #shape为[16, ?]\n",
    "    shape = tf.cast(parsed_features[\"shape\"], tf.int32)\n",
    "    \n",
    "    decode_imgs = []\n",
    "    for i in range(16):\n",
    "        decode_img = tf.image.decode_jpeg(encode_bytes[i], channels=3) #把压缩编码的jpg字节码解码(不需要reshape，解码会自动解析图片大小)\n",
    "        decode_imgs.append(decode_img)\n",
    "        \n",
    "    decode_imgs = tf.convert_to_tensor(decode_imgs, tf.float32)\n",
    "    \n",
    "    width = tf.cast(shape[0], dtype=tf.float32)\n",
    "    height = tf.cast(shape[1], dtype=tf.float32)\n",
    "        \n",
    "    #decode_imgs = tf.random_crop(decode_imgs, [16, tf.cast(height, dtype=tf.int32), tf.cast(width, dtype=tf.int32), 3])\n",
    "    \n",
    "    \n",
    "    def crop_h():\n",
    "        scale=tf.cast(float(160)/width*height+1.0, dtype=tf.int32)\n",
    "        rimg = tf.image.resize_images(img, (scale, 160)) \n",
    "        crop_h=(scale-160)/2\n",
    "        return rimg[crop_h:crop_h+160,:,:] \n",
    "            \n",
    " \n",
    "    def crop_w():\n",
    "        scale=tf.cast(float(160)/height*width+1.0, dtype=tf.int32)\n",
    "        rimg = tf.image.resize_images(img, (160, scale))\n",
    "        crop_w=(scale-160)/2\n",
    "        return rimg[:,crop_w:crop_w+160,:] \n",
    "    \n",
    "    ret_imgs = []\n",
    "    for i in range(16):\n",
    "        #此处可做预处理\n",
    "    \n",
    "        img = decode_imgs[i]\n",
    "        img = tf.cond(width>height, crop_w, crop_h)\n",
    "        #img = tf.cond(tf.equal(flip, 0), lambda:img, lambda:tf.image.flip_left_right(img))\n",
    "        #img = tf.image.per_image_standardization(img)\n",
    "        \n",
    "        ret_imgs.append(img)\n",
    "        \n",
    "    x = tf.cast(tf.convert_to_tensor(ret_imgs), tf.float32)\n",
    "    y = tf.cast(parsed_features[\"label\"], tf.int32)\n",
    "    return x, y\n",
    "\n",
    "def my_input_fn(file_path, shuffle=True, shuffle_buffer_size=1000, batch_size=10): \n",
    "    dataset = tf.data.TFRecordDataset(file_path).map(parse)\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size) #注意suffle_buffer_size过大会使得内存爆满，过小会使得shuffle程度过低（尤其是按label顺序的写入tfrecord)\n",
    "    dataset = dataset.repeat() #保证读到数据集末端不会越界（重新开始下个epoch)\n",
    "    dataset = dataset.batch(batch_size) #注意batch内需要个样本shape相同，所以需要保证已经做好预处理\n",
    "    iterator = dataset.make_one_shot_iterator() #生成迭代器\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretreat(inputs):\n",
    "        \n",
    "    crop_size = CROP_SIZE\n",
    "    outputs = []\n",
    "    for imggroup in inputs:\n",
    "            \n",
    "        group = []\n",
    "        for image in imggroup:\n",
    "                \n",
    "            height = image.shape[0]\n",
    "            width = image.shape[1]   \n",
    "                        \n",
    "            if(width>height):\n",
    "                scale=float(crop_size)/float(height)\n",
    "                img=np.array(cv2.resize(image,(int(width*scale+1),crop_size))).astype(np.float32)      \n",
    "            else:                    \n",
    "                scale=float(crop_size)/float(width)\n",
    "                img=np.array(cv2.resize(image,(crop_size,int(height*scale+1)))).astype(np.float32)\n",
    "                \n",
    "            crop_y=int((img.shape[0]-crop_size)/2)\n",
    "            crop_x=int((img.shape[1]-crop_size)/2)\n",
    "            img=img[crop_y:crop_y+crop_size,crop_x:crop_x+crop_size,:]\n",
    "\n",
    "            #print img\n",
    "            \n",
    "            std = np.std(img, ddof=1)\n",
    "            mean = np.mean(img)\n",
    "            std = np.max([std, 1.0/np.sqrt(160*160*3)])\n",
    "           \n",
    "            img = (img-mean)/std\n",
    "\n",
    "            group.append(img)\n",
    "            \n",
    "        outputs.append(group)\n",
    "    return np.array(outputs).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_serving(inputs, outputs, sess, saver, export_version, export_path): \n",
    "    inputs = tf.saved_model.utils.build_tensor_info(inputs)\n",
    "    outputs = tf.saved_model.utils.build_tensor_info(outputs)\n",
    "     \n",
    "    signature = tf.saved_model.signature_def_utils.build_signature_def( \n",
    "        inputs={'x': inputs}, outputs={'logit': outputs}, \n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME) \n",
    "    print tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    export_path = export_path+'/'+export_version\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(export_path) \n",
    "    main_op = tf.group(tf.tables_initializer(), name='main_op')\n",
    "    builder.add_meta_graph_and_variables(sess=sess, \n",
    "                                         tags=[tf.saved_model.tag_constants.SERVING], \n",
    "                                         signature_def_map={ 'predict_image': signature}, \n",
    "                                         main_op=main_op,\n",
    "                                         saver=saver) \n",
    "    builder.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107 1107\n",
      "INFO:tensorflow:Restoring parameters from ./SC_DA_P3D_RES1010_30000-30000\n",
      "tensorflow/serving/predict\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./3daction/1111/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "#Cell for Testing singal\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "tf.reset_default_graph()\n",
    "#when testing ,make sure IS_TRAIN==False,or you will get bad result for testing.\n",
    "IS_TRAIN=False\n",
    "BATCH_SIZE=1\n",
    "final_acc=0\n",
    "IS_DA=False\n",
    "\n",
    "c=0\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    global_step=tf.get_variable('global_step',[],initializer=tf.constant_initializer(0),trainable=False)\n",
    "    input_placeholder=tf.placeholder(tf.float32,shape=(BATCH_SIZE,NUM_FRAMES_PER_CLIP,CROP_SIZE,CROP_SIZE,RGB_CHANNEL))\n",
    "    label_placeholder=tf.placeholder(tf.int64,shape=(BATCH_SIZE))\n",
    "    #when testing,make sure dropout=1.0(keep_prob)\n",
    "    logit, xx=inference_p3d(input_placeholder,BATCH_SIZE)\n",
    "    variable_averages=tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,num_updates=global_step)\n",
    "\n",
    "    acc=compute_accuracy(logit,label_placeholder)\n",
    "    init=tf.global_variables_initializer()\n",
    "    variable_avg_restore=variable_averages.variables_to_restore()\n",
    "    print len(variable_avg_restore), len(tf.global_variables())\n",
    "    saver=tf.train.Saver(variable_avg_restore)\n",
    "    \n",
    "    config = tf.ConfigProto()  \n",
    "    config.gpu_options.allow_growth=True\n",
    "    sess=tf.Session(config=config)\n",
    "    sess.run(init)\n",
    "    #restore your checkpoint file\n",
    "    saver.restore(sess,'./SC_DA_P3D_RES1010_30000-30000')\n",
    "\n",
    "    save_model_to_serving(input_placeholder, logit, sess, saver, '1111', './3daction')\n",
    "    '''\n",
    "    ls = os.listdir('/home/sw/data/2018-11-01/DS-2PT7D20IW-DE20170902AACH828556927/Cameral1')\n",
    "    for step in range(1):\n",
    "        #image,label = sess.run(test_next_batch)\n",
    "        dir_path = '/home/sw/Work/tf/yaochang/tests/'\n",
    "        inputs = []\n",
    "        for i in range(16):\n",
    "            file_path = dir_path + '{:0>4}'.format(i) + '.jpg'\n",
    "            img = Image.open(file_path)\n",
    "            inputs.append(np.array(img))\n",
    "\n",
    "        inputs = np.array(inputs)\n",
    "        image = pretreat(np.expand_dims(inputs, 0))\n",
    "        print np.std(image[0][0], ddof=1), np.mean(image[0][0])\n",
    "        label = np.array([0])\n",
    "\n",
    "        accuracy, pred, xx=sess.run([acc,logit, xx],feed_dict={input_placeholder:image,\n",
    "                                    label_placeholder:label})\n",
    "\n",
    "        print '->', np.argmax(pred, 1), pred, np.sum(np.fabs(xx))\n",
    "        final_acc+=accuracy\n",
    "        c+=1\n",
    "    print(final_acc/c)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
